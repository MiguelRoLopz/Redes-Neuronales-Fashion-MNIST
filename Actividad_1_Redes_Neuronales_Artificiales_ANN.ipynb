{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiDzBoKGwmMZ"
      },
      "source": [
        "# REDES NEURONALES\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "En esta actividad vamos a utilizar una red neuronal para clasificar imágenes de prendas de ropa. Para ello, utilizaremos Keras con TensorFlow.\n",
        "\n",
        "El dataset a utilizar es Fashion MNIST, un problema sencillo con imágenes pequeñas de ropa, pero más interesante que el dataset de MNIST. Puedes consultar más información sobre el dataset en [este enlace](https://github.com/zalandoresearch/fashion-mnist).\n",
        "\n",
        "El código utilizado para contestar tiene que quedar claramente reflejado en el Notebook. Puedes crear nuevas celdas si así lo deseas para estructurar tu código y sus salidas. A la hora de entregar el notebook, **asegúrate de que los resultados de ejecutar tu código han quedado guardados**. Por ejemplo, a la hora de entrenar una red neuronal tiene que verse claramente un log de los resultados de cada epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "gSHr268SwmMa"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.initializers import Zeros, RandomNormal, GlorotUniform\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zScMKU2OKSPD"
      },
      "source": [
        "En primer lugar vamos a importar el dataset Fashion MNIST (recordad que este es uno de los dataset de entranamiento que estan guardados en keras) que es el que vamos a utilizar en esta actividad:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "4voG2hxxG4h3"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.fashion_mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JphLsCvgKrzb"
      },
      "source": [
        "Llamar a **load_data** en este dataset nos dará dos conjuntos de dos listas, estos serán los valores de entrenamiento y prueba para los gráficos que contienen las prendas de vestir y sus etiquetas.\n",
        "\n",
        "Nota: Aunque en esta actividad lo veis de esta forma, también lo vais a poder encontrar como 4 variables de esta forma: training_images, training_labels, test_images, test_labels = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1muD4PHEG4h6",
        "outputId": "2f6beb46-3176-4adf-a64b-6dab9ea81bbe"
      },
      "outputs": [],
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWGpJqVVLT3Y"
      },
      "source": [
        "Antes de continuar vamos a dar un vistazo a nuestro dataset. Para ello, vamos a ver una imagen de entrenamiento y su etiqueta o clase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "id": "t5a5PlswG4h8",
        "outputId": "2edeb68d-fcba-4f20-c49a-f80a5c51b012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0   0   1   4   0   0   0   0   1   1   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62  54   0   0   0   1   3   4   0   0   3]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134 144 123  23   0   0   0   0  12  10   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178 107 156 161 109  64  23  77 130  72  15]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216 216 163 127 121 122 146 141  88 172  66]\n",
            " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229 223 223 215 213 164 127 123 196 229   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228 235 227 224 222 224 221 223 245 173   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198 180 212 210 211 213 223 220 243 202   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192 169 227 208 218 224 212 226 197 209  52]\n",
            " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203 198 221 215 213 222 220 245 119 167  56]\n",
            " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240 232 213 218 223 234 217 217 209  92   0]\n",
            " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219 222 221 216 223 229 215 218 255  77   0]\n",
            " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208 211 218 224 223 219 215 224 244 159   0]\n",
            " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230 224 234 176 188 250 248 233 238 215   0]\n",
            " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223 255 255 221 234 221 211 220 232 246   0]\n",
            " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221 188 154 191 210 204 209 222 228 225   0]\n",
            " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117 168 219 221 215 217 223 223 224 229  29]\n",
            " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245 239 223 218 212 209 222 220 221 230  67]\n",
            " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216 199 206 186 181 177 172 181 205 206 115]\n",
            " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191 195 191 198 192 176 156 167 177 210  92]\n",
            " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209 210 210 211 188 188 194 192 216 170   0]\n",
            " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179 182 182 181 176 166 168  99  58   0   0]\n",
            " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "np.set_printoptions(linewidth=200)\n",
        "plt.imshow(training_images[0], cmap=\"gray\") # recordad que siempre es preferible trabajar en blanco y negro\n",
        "\n",
        "print(training_labels[0])\n",
        "print(training_images[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCJvZx3MLucY"
      },
      "source": [
        "Habreis notado que todos los valores numericos están entre 0 y 255. Si estamos entrenando una red neuronal, una buena practica es transformar todos los valores entre 0 y 1, un proceso llamado \"normalización\" y afortunadamente en Python es fácil normalizar una lista. Lo puedes hacer de esta manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "tojL1BmjG4h_"
      },
      "outputs": [],
      "source": [
        "training_images  = training_images / 255.0\n",
        "test_images = test_images / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaqXlSMBwmMg"
      },
      "source": [
        "## 1. Información sobre el dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0aer8ZZwmMh"
      },
      "source": [
        "Una vez tenemos los datos cargados en memoria, vamos a obtener información sobre los mismos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-im9PnEwmMh"
      },
      "source": [
        "**Pregunta 1.1 *(0.25 puntos)*** ¿Cuántas imágenes hay de *training* y de *test*? ¿Qué tamaño tienen las imágenes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "lvP0Y4SCwmMi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de imágenes de entrenamiento: 60000\n",
            "\n",
            "Número de imágenes de test: 10000\n",
            "\n",
            "Dimensiones de cada imagen: (28, 28)\n"
          ]
        }
      ],
      "source": [
        "num_training_images = len(training_images)\n",
        "num_test_images = len(test_images)\n",
        "\n",
        "dim_images = training_images[0].shape\n",
        "\n",
        "print('Número de imágenes de entrenamiento:', num_training_images)\n",
        "print('\\nNúmero de imágenes de test:', num_test_images)\n",
        "print('\\nDimensiones de cada imagen:', dim_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xwp5ljFKwmMj"
      },
      "source": [
        "En primer lugar, calculamos el número de imágenes que hay en los conjuntos de entrenamiento y de test midiendo la longitud de ambos valores normalizados.\n",
        "\n",
        "Tras ello, obtenemos las dimensiones de las imágenes de entrenamiento, asumiendo que todas tienen la misma dimensión. Para ello, accedemos a la primera imagen del conjunto de entrenamiento y utilizamos el atributo 'shape' para obtener las dimensiones de la imagen.\n",
        "\n",
        "Por último, sacamos los resultados por pantalla."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2LsvfHOwmMk"
      },
      "source": [
        "**Pregunta 1.2 *(0.25 puntos)*** Realizar una exploración de las variables que contienen los datos. Describir en qué consiste un example del dataset (qué información se guarda en cada imagen) y describir qué contiene la información en y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "3W5rzaGxwmMk"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de píxeles de la imagen de ejemplo\n",
            " [[0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.00392157 0.         0.         0.05098039 0.28627451\n",
            "  0.         0.         0.00392157 0.01568627 0.         0.         0.         0.         0.00392157 0.00392157 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.01176471 0.         0.14117647 0.53333333 0.49803922\n",
            "  0.24313725 0.21176471 0.         0.         0.         0.00392157 0.01176471 0.01568627 0.         0.         0.01176471]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.02352941 0.         0.4        0.8        0.69019608\n",
            "  0.5254902  0.56470588 0.48235294 0.09019608 0.         0.         0.         0.         0.04705882 0.03921569 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.60784314 0.9254902  0.81176471\n",
            "  0.69803922 0.41960784 0.61176471 0.63137255 0.42745098 0.25098039 0.09019608 0.30196078 0.50980392 0.28235294 0.05882353]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.00392157 0.         0.27058824 0.81176471 0.8745098  0.85490196\n",
            "  0.84705882 0.84705882 0.63921569 0.49803922 0.4745098  0.47843137 0.57254902 0.55294118 0.34509804 0.6745098  0.25882353]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.00392157 0.00392157 0.00392157 0.         0.78431373 0.90980392 0.90980392 0.91372549\n",
            "  0.89803922 0.8745098  0.8745098  0.84313725 0.83529412 0.64313725 0.49803922 0.48235294 0.76862745 0.89803922 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.71764706 0.88235294 0.84705882 0.8745098\n",
            "  0.89411765 0.92156863 0.89019608 0.87843137 0.87058824 0.87843137 0.86666667 0.8745098  0.96078431 0.67843137 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.75686275 0.89411765 0.85490196 0.83529412\n",
            "  0.77647059 0.70588235 0.83137255 0.82352941 0.82745098 0.83529412 0.8745098  0.8627451  0.95294118 0.79215686 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.00392157 0.01176471 0.         0.04705882 0.85882353 0.8627451  0.83137255 0.85490196\n",
            "  0.75294118 0.6627451  0.89019608 0.81568627 0.85490196 0.87843137 0.83137255 0.88627451 0.77254902 0.81960784 0.20392157]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.02352941 0.         0.38823529 0.95686275 0.87058824 0.8627451  0.85490196\n",
            "  0.79607843 0.77647059 0.86666667 0.84313725 0.83529412 0.87058824 0.8627451  0.96078431 0.46666667 0.65490196 0.21960784]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.01568627 0.         0.         0.21568627 0.9254902  0.89411765 0.90196078 0.89411765\n",
            "  0.94117647 0.90980392 0.83529412 0.85490196 0.8745098  0.91764706 0.85098039 0.85098039 0.81960784 0.36078431 0.        ]\n",
            " [0.         0.         0.00392157 0.01568627 0.02352941 0.02745098 0.00784314 0.         0.         0.         0.         0.         0.92941176 0.88627451 0.85098039 0.8745098  0.87058824\n",
            "  0.85882353 0.87058824 0.86666667 0.84705882 0.8745098  0.89803922 0.84313725 0.85490196 1.         0.30196078 0.        ]\n",
            " [0.         0.01176471 0.         0.         0.         0.         0.         0.         0.         0.24313725 0.56862745 0.8        0.89411765 0.81176471 0.83529412 0.86666667 0.85490196\n",
            "  0.81568627 0.82745098 0.85490196 0.87843137 0.8745098  0.85882353 0.84313725 0.87843137 0.95686275 0.62352941 0.        ]\n",
            " [0.         0.         0.         0.         0.07058824 0.17254902 0.32156863 0.41960784 0.74117647 0.89411765 0.8627451  0.87058824 0.85098039 0.88627451 0.78431373 0.80392157 0.82745098\n",
            "  0.90196078 0.87843137 0.91764706 0.69019608 0.7372549  0.98039216 0.97254902 0.91372549 0.93333333 0.84313725 0.        ]\n",
            " [0.         0.22352941 0.73333333 0.81568627 0.87843137 0.86666667 0.87843137 0.81568627 0.8        0.83921569 0.81568627 0.81960784 0.78431373 0.62352941 0.96078431 0.75686275 0.80784314\n",
            "  0.8745098  1.         1.         0.86666667 0.91764706 0.86666667 0.82745098 0.8627451  0.90980392 0.96470588 0.        ]\n",
            " [0.01176471 0.79215686 0.89411765 0.87843137 0.86666667 0.82745098 0.82745098 0.83921569 0.80392157 0.80392157 0.80392157 0.8627451  0.94117647 0.31372549 0.58823529 1.         0.89803922\n",
            "  0.86666667 0.7372549  0.60392157 0.74901961 0.82352941 0.8        0.81960784 0.87058824 0.89411765 0.88235294 0.        ]\n",
            " [0.38431373 0.91372549 0.77647059 0.82352941 0.87058824 0.89803922 0.89803922 0.91764706 0.97647059 0.8627451  0.76078431 0.84313725 0.85098039 0.94509804 0.25490196 0.28627451 0.41568627\n",
            "  0.45882353 0.65882353 0.85882353 0.86666667 0.84313725 0.85098039 0.8745098  0.8745098  0.87843137 0.89803922 0.11372549]\n",
            " [0.29411765 0.8        0.83137255 0.8        0.75686275 0.80392157 0.82745098 0.88235294 0.84705882 0.7254902  0.77254902 0.80784314 0.77647059 0.83529412 0.94117647 0.76470588 0.89019608\n",
            "  0.96078431 0.9372549  0.8745098  0.85490196 0.83137255 0.81960784 0.87058824 0.8627451  0.86666667 0.90196078 0.2627451 ]\n",
            " [0.18823529 0.79607843 0.71764706 0.76078431 0.83529412 0.77254902 0.7254902  0.74509804 0.76078431 0.75294118 0.79215686 0.83921569 0.85882353 0.86666667 0.8627451  0.9254902  0.88235294\n",
            "  0.84705882 0.78039216 0.80784314 0.72941176 0.70980392 0.69411765 0.6745098  0.70980392 0.80392157 0.80784314 0.45098039]\n",
            " [0.         0.47843137 0.85882353 0.75686275 0.70196078 0.67058824 0.71764706 0.76862745 0.8        0.82352941 0.83529412 0.81176471 0.82745098 0.82352941 0.78431373 0.76862745 0.76078431\n",
            "  0.74901961 0.76470588 0.74901961 0.77647059 0.75294118 0.69019608 0.61176471 0.65490196 0.69411765 0.82352941 0.36078431]\n",
            " [0.         0.         0.29019608 0.74117647 0.83137255 0.74901961 0.68627451 0.6745098  0.68627451 0.70980392 0.7254902  0.7372549  0.74117647 0.7372549  0.75686275 0.77647059 0.8\n",
            "  0.81960784 0.82352941 0.82352941 0.82745098 0.7372549  0.7372549  0.76078431 0.75294118 0.84705882 0.66666667 0.        ]\n",
            " [0.00784314 0.         0.         0.         0.25882353 0.78431373 0.87058824 0.92941176 0.9372549  0.94901961 0.96470588 0.95294118 0.95686275 0.86666667 0.8627451  0.75686275 0.74901961\n",
            "  0.70196078 0.71372549 0.71372549 0.70980392 0.69019608 0.65098039 0.65882353 0.38823529 0.22745098 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.15686275 0.23921569 0.17254902 0.28235294 0.16078431 0.1372549  0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]]\n",
            "\n",
            "Etiqueta de la imagen de ejemplo: 9\n"
          ]
        }
      ],
      "source": [
        "image_1 = training_images[0]\n",
        "label_1 = training_labels[0]\n",
        "\n",
        "plt.imshow(image_1, cmap=\"gray\")\n",
        "plt.title('Imagen de ejemplo para exploración de los datos\\n')\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "print('Matriz de píxeles de la imagen de ejemplo\\n', image_1)\n",
        "print('\\nEtiqueta de la imagen de ejemplo:', label_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaEWKFyvwmMm"
      },
      "source": [
        "En la celda anterior, seleccionamos la primera imagen para realizar la exploración de los datos y los mostramos por pantalla.\n",
        "\n",
        "Cada una de las imágenes del dataset Fashion MNIST es una matriz de dimensiones 28x28 píxeles. Cada elemento de la matriz representa la intensidad de un píxel en escala de grises, donde 0 es negro y 255 blanco. Las escalas de grises son utilizadas normalmente en tareas de visión artificial para simplificar el procesamiento, siempre y cuando el color no nos aporte información crucial para el análisis.\n",
        "\n",
        "La etiqueta (y) es un número entero entre 0 y 9 que corresponde con un tipo de prenda específico en el dataset. Según el repositorio de GitHub donde se muestra toda la información relativa al dataset Fashion MNIST (facilitado al inicio de este Notebook), cada número corresponde con la siguiente prenda: 0- T-shirt/Tops; 1- Trousers; 2- Pullover; 3- Dress; 4- Coat; 5- Sandal; 6- Shirt; 7- Sneaker; 8- Bag; 9- Ankle boot\n",
        "\n",
        "Como podemos comprobar, nuestro código ha clasificado correctamente las Nike Air Jordan como 'bota o zapatilla tobillera'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI3IAhOQ8zHi"
      },
      "source": [
        "## 2. Creación del Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYUWWsszMAKt"
      },
      "source": [
        "Ahora vamos a definir el modelo, pero antes vamos a repasar algunos comandos y conceptos muy útiles:\n",
        "* **Sequential**: Eso define una SECUENCIA de capas en la red neuronal\n",
        "* **Dense**: Añade una capa de neuronas\n",
        "* **Flatten**: ¿Recuerdas cómo eran las imágenes cuando las imprimiste para poder verlas? Un cuadrado, Flatten toma ese cuadrado y lo convierte en un vector de una dimensión.\n",
        "\n",
        "Cada capa de neuronas necesita una función de activación. Normalmente se usa la función relu en las capas intermedias y softmax en la ultima capa (en problemas de clasificación de más de dos items)\n",
        "* **Relu** significa que \"Si X>0 devuelve X, si no, devuelve 0\", así que lo que hace es pasar sólo valores 0 o mayores a la siguiente capa de la red.\n",
        "* **Softmax** toma un conjunto de valores, y escoge el más grande."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgBW1yE2MwPp"
      },
      "source": [
        " **Pregunta 2.1 (2 puntos)**. Utilizando Keras, y preparando los datos de X e y como fuera necesario, define y entrena una red neuronal que sea capaz de clasificar imágenes de Fashion MNIST con las siguientes características:\n",
        "\n",
        "* Una hidden layer de tamaños 128, utilizando unidades sigmoid\n",
        "Optimizador Adam.\n",
        "* Durante el entrenamiento, la red tiene que mostrar resultados de loss y accuracy por cada epoch.\n",
        "* La red debe entrenar durante 10 epochs y batch size de 64.\n",
        "* La última capa debe de ser una capa softmax.\n",
        "* Tu red tendría que ser capaz de superar fácilmente 80% de accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "aTaD2QXIORwu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7670 - loss: 0.6705 - val_accuracy: 0.8480 - val_loss: 0.4267\n",
            "Epoch 2/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8603 - loss: 0.3963 - val_accuracy: 0.8593 - val_loss: 0.4005\n",
            "Epoch 3/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8709 - loss: 0.3566 - val_accuracy: 0.8628 - val_loss: 0.3907\n",
            "Epoch 4/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8816 - loss: 0.3241 - val_accuracy: 0.8702 - val_loss: 0.3607\n",
            "Epoch 5/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8889 - loss: 0.3062 - val_accuracy: 0.8703 - val_loss: 0.3664\n",
            "Epoch 6/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8931 - loss: 0.2898 - val_accuracy: 0.8835 - val_loss: 0.3345\n",
            "Epoch 7/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8955 - loss: 0.2785 - val_accuracy: 0.8795 - val_loss: 0.3380\n",
            "Epoch 8/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9031 - loss: 0.2640 - val_accuracy: 0.8801 - val_loss: 0.3328\n",
            "Epoch 9/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9053 - loss: 0.2567 - val_accuracy: 0.8843 - val_loss: 0.3341\n",
            "Epoch 10/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9100 - loss: 0.2438 - val_accuracy: 0.8800 - val_loss: 0.3351\n"
          ]
        }
      ],
      "source": [
        "# Definimos el modelo\n",
        "model = Sequential([Flatten(input_shape=(28, 28)), # Aplanamos la entrada de imágenes 28x28 a un vector\n",
        "                    Dense(128, activation='relu'), # Capa oculta con 128 neuronas y activación relu\n",
        "                    Dense(10, activation='softmax')]) # Capa de salida con activación softmax para 10 clases\n",
        "\n",
        "# Compilamos el modelo\n",
        "model.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Entrenamos el modelo\n",
        "entrenamiento = model.fit(training_images, training_labels, epochs=10, batch_size=64, validation_data=(test_images, test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para definir y entrenar la red neuronal, en primer lugar, debemos incluir 'Flatten' en la importación de bibliotecas al inicio del Notebook, ya que no estaba añadida > from keras.layers import Dense, Flatten\n",
        "\n",
        "Ahora, podemos definir el modelo que consta de las siguientes partes:\n",
        "\n",
        "· Uso de Flatten en la entrada para convertir las imágenes de 28x28 píxeles en un vector unidimensional para poder procesarlas en la capa densa.\n",
        "· Utilizamos una capa oculta con Dense con 128 neuronas y función de activación ReLU para activar la neurona sólo si la entrada es positiva, o cero en caso contrario como se ha definido anteriormente.\n",
        "· Finalizamos con otra capa Dense con 10 neuronas, que corresponden a cada uno de los tipos de ropa que contiene el dataset, utilizando una función softmax para obtener probabilidades.\n",
        "\n",
        "Tras formar el modelo, lo compilamos utilizando un optimizador Adam, una función de pérdida sparse_categorical_crossentropy (adecuada para etiquetas de clases de tipo entero) y midiendo la precisión (accuracy) durante el entrenamiento del modelo para monitorizar el rendimiento del mismo.\n",
        "\n",
        "Por último, entrenamos el modelo durante 10 epochs con un tamaño de lote igual a 64. Durante el entrenamiento también validamos el modelo usando los datos de test.\n",
        "\n",
        "Como podemos observar al ejecutar el código, los datos de accuracy van creciendo conforme se aportan más datos de entrenamiento al modelo. Estos son superiores al 80% exigido en la práctica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxr5hTKYOQnK"
      },
      "source": [
        "Para concluir el entrenamiento de la red neuronal, una buena practica es evaluar el modelo para ver si la precisión de entrenamiento es real\n",
        "\n",
        "**pregunta 2.2 (0.5 puntos)**: evalua el modelo con las imagenes y etiquetas test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "VNjQEtUUG4iI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - accuracy: 0.8804 - loss: 0.3400\n",
            "Test accuracy: 0.8799999952316284\n",
            "Test loss: 0.33505600690841675\n"
          ]
        }
      ],
      "source": [
        "# Evaluamos el modelo en el conjunto de test\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print('Test accuracy:', test_accuracy)\n",
        "print('Test loss:', test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilizamos el método 'evaluate' para enviar a nuestro modelo las imágenes y etiquetas de test para calcular la pérdida y precisión del modelo en el dataset.\n",
        "\n",
        "Estos resultados son importantes para evaluar cómo el modelo maneja los datos que no se utilizaron durante el entrenamiento, lo que nos proporciona una estimación sobre cómo se comportará nuestro modelo ante nuevos datos generales, es decir, para no incurrir en que nuestro modelo únicamente funcione con los datos iniciales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygMVnmSYO83U"
      },
      "source": [
        "Ahora vamos a explorar el código con una serie de ejercicios para alcanzar un grado de comprensión mayor sobre las redes neuronales y su entrenamiento.\n",
        "\n",
        "## 3: Funcionamiento de la predicción de la red neuronal\n",
        "\n",
        "Sigue los siguientes pasos: \n",
        "\n",
        "* Crea una variable llamada **classifications** para construir un clasificador con las imágenes de prueba, para ello puedes utilizar la función predict sobre el conjunto de test\n",
        "* Imprime con la función print la primera entrada en las clasificaciones. \n",
        "\n",
        "**pregunta 3.1 (0.25 puntos)**, el resultado al imprimirlo es un vector de números, \n",
        "* ¿Por qué crees que ocurre esto, y qué representa este vector de números?\n",
        "\n",
        "**pregunta 3.2 (0.25 puntos)**\n",
        "* ¿Cúal es la clase de la primera entrada#  de la variable **classifications**? La respuesta puede ser un número o su etiqueta/clase equivalente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "b-mL-h4xQhCm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step\n",
            "Clasificación con 128 neuronas:\n",
            " [1.1136519e-06 3.4594724e-08 5.2151663e-06 5.8225424e-07 1.2393685e-08 6.6618505e-04 3.6788563e-07 1.1147785e-02 1.7586530e-06 9.8817700e-01]\n"
          ]
        }
      ],
      "source": [
        "classifications = model.predict(test_images)\n",
        "print('Clasificación con 128 neuronas:\\n', classifications[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvbVC9gaQhMY"
      },
      "source": [
        "El resultado tras imprimir la variable del clasificador es un vector de 10 números que corresponde a la probabilidad de que la imagen de entrada (en este caso la primera) corresponda a una de los 10 tipos de prendas de ropa del dataset Fashion MNIST. Cada índice del vector representa una clase específica del dataset. El valor en cada posición del vector indica la probabilidad de que la imagen pertenezca a la clase correspondiente a dicho índice. La clase predicha por el modelo para una imagen será la que tenga un valor superior en el vector de probabilidades."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRWo-75tdgv0"
      },
      "source": [
        "En nuestro caso, la clase que tiene mayor probabilidad es el último índice del vector, que corresponde a la clase 'bota o zapatilla tobillera'. El modelo arroja un 99% de probabilidad de que la imagen analizada corresponde a esta categoría de ropa.\n",
        "\n",
        "Para casos donde los índices sean muy parejos y para no dejarnos la vista analizando los valores exponenciales que se muestran en el vector, podríamos utilizar la función de numpy '.argmax()' para encontrar de forma más sencilla el valor máximo en el vector de probabilidades."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiQ8qAzhRQ4L"
      },
      "source": [
        "# **4: Impacto variar el número de neuronas en las capas ocultas**\n",
        "\n",
        "En este ejercicio vamos a experimentar con nuestra red neuronal cambiando el numero de neuronas por 512 y por 1024. Para ello, utiliza la red neuronal de la pregunta 1, y su capa oculta cambia las 128 neuronas:\n",
        "\n",
        "* **512 neuronas en la capa oculta\n",
        "* **1024 neuronas en la capa oculta\n",
        "\n",
        "y entrena la red en ambos casos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "cdP8ZwuaUV93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7909 - loss: 0.6035 - val_accuracy: 0.8376 - val_loss: 0.4426\n",
            "Epoch 2/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8652 - loss: 0.3744 - val_accuracy: 0.8630 - val_loss: 0.3777\n",
            "Epoch 3/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8799 - loss: 0.3294 - val_accuracy: 0.8694 - val_loss: 0.3573\n",
            "Epoch 4/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8905 - loss: 0.3016 - val_accuracy: 0.8712 - val_loss: 0.3586\n",
            "Epoch 5/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8952 - loss: 0.2793 - val_accuracy: 0.8795 - val_loss: 0.3402\n",
            "Epoch 6/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9022 - loss: 0.2670 - val_accuracy: 0.8726 - val_loss: 0.3508\n",
            "Epoch 7/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9056 - loss: 0.2571 - val_accuracy: 0.8852 - val_loss: 0.3281\n",
            "Epoch 8/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9108 - loss: 0.2386 - val_accuracy: 0.8891 - val_loss: 0.3178\n",
            "Epoch 9/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9127 - loss: 0.2316 - val_accuracy: 0.8880 - val_loss: 0.3215\n",
            "Epoch 10/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9167 - loss: 0.2258 - val_accuracy: 0.8870 - val_loss: 0.3259\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - accuracy: 0.8857 - loss: 0.3318\n",
            "\n",
            "Test accuracy: 0.8870000243186951\n",
            "Test loss: 0.32585975527763367\n"
          ]
        }
      ],
      "source": [
        "# Definimos el modelo\n",
        "model_512 = Sequential([Flatten(input_shape=(28, 28)), # Aplanamos la entrada de imágenes 28x28 a un vector\n",
        "                    Dense(512, activation='relu'), # Capa oculta con 512 neuronas y activación relu\n",
        "                    Dense(10, activation='softmax')]) # Capa de salida con activación softmax para 10 clases\n",
        "\n",
        "# Compilamos el modelo\n",
        "model_512.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Entrenamos el modelo\n",
        "entrenamiento = model_512.fit(training_images, training_labels, epochs=10, batch_size=64, validation_data=(test_images, test_labels))\n",
        "\n",
        "# Evaluamos el modelo en el conjunto de test\n",
        "test_loss, test_accuracy = model_512.evaluate(test_images, test_labels)\n",
        "\n",
        "print('\\nTest accuracy:', test_accuracy)\n",
        "print('Test loss:', test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "YXBlbbfuUaPa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7970 - loss: 0.5851 - val_accuracy: 0.8483 - val_loss: 0.4170\n",
            "Epoch 2/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8681 - loss: 0.3657 - val_accuracy: 0.8602 - val_loss: 0.3854\n",
            "Epoch 3/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8846 - loss: 0.3173 - val_accuracy: 0.8619 - val_loss: 0.3846\n",
            "Epoch 4/10\n",
            "\u001b[1m916/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8915 - loss: 0.2943"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[85], line 12\u001b[0m\n\u001b[0;32m      7\u001b[0m model_1024\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m                 loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m                 metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Entrenamos el modelo\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m entrenamiento \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_1024\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Evaluamos el modelo en el conjunto de test\u001b[39;00m\n\u001b[0;32m     15\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m model_1024\u001b[38;5;241m.\u001b[39mevaluate(test_images, test_labels)\n",
            "File \u001b[1;32mc:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
            "File \u001b[1;32mc:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[1;32mc:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
            "File \u001b[1;32mc:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Definimos el modelo\n",
        "model_1024 = Sequential([Flatten(input_shape=(28, 28)), # Aplanamos la entrada de imágenes 28x28 a un vector\n",
        "                    Dense(1024, activation='relu'), # Capa oculta con 1024 neuronas y activación relu\n",
        "                    Dense(10, activation='softmax')]) # Capa de salida con activación softmax para 10 clases\n",
        "\n",
        "# Compilamos el modelo\n",
        "model_1024.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Entrenamos el modelo\n",
        "entrenamiento = model_1024.fit(training_images, training_labels, epochs=10, batch_size=64, validation_data=(test_images, test_labels))\n",
        "\n",
        "# Evaluamos el modelo en el conjunto de test\n",
        "test_loss, test_accuracy = model_1024.evaluate(test_images, test_labels)\n",
        "\n",
        "print('\\nTest accuracy:', test_accuracy)\n",
        "print('Test loss:', test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG0h2HL-Uj93"
      },
      "source": [
        "**pregunta 4.1 (0.5 puntos)**: ¿Cual es el impacto que tiene la red neuronal? \n",
        "\n",
        "Al cambiar el número de neuronas de la capa oculta y entrenar el modelo, podemos observar como los tiempos de respuesta aumentan de forma proporcional cuantas más neuronas existen en la red. Estos modelos requieren de más memoria y tiempo de computación, aunque en este caso no suponen ningún problema.\n",
        "\n",
        "Ahora bien, si además de formar y entrenar las nuevas redes neuronales, evaluamos el rendimiento del modelo, podemos comprobar como los valores de accuracy en la red de 512 neuronas en la capa oculta es ligeramente superior que en la de 1024. Esto puede deberse a que se esté sobreajustando el modelo (overfitting) por aumentar demasiado el número de neuronas, lo que puede reducir la precisión. Gracias a la evaluación del modelo podemos entender mejor los valores de rendimiento del modelo (no sólo viendo el desarrollo del entrenamiento)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-NpUI9EVkVz"
      },
      "source": [
        "Si ahora entrenais el modelo de esta forma (con 512 y 1024 neuronas en la capa oculta) y volveis a ejecutar el predictor guardado en la variable **classifications**, escribir el código del clasificador del ejercicio 1 de nuevo e imprimid el primer objeto guardado en la variable classifications.\n",
        "\n",
        "**pregunta 4.2 (0.25 puntos)**: \n",
        "\n",
        "* ¿En que clase esta clasificado ahora la primera prenda de vestir de la variable classifications?\n",
        "\n",
        "**pregunta 4.3 (0.25 puntos)**: \n",
        "\n",
        "* ¿Porque crees que ha ocurrido esto?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdJHl3V-G4iS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step\n",
            "Clasificación con 512 neuronas:\n",
            " [8.7733270e-07 3.5910794e-08 7.8748776e-08 3.6780420e-08 3.5304151e-06 5.8638211e-02 1.9183221e-06 2.3564139e-02 4.2028537e-07 9.1779083e-01]\n",
            "\n",
            "Clase predicha por el modelo con 512 neuronas en la capa oculta: 9\n"
          ]
        }
      ],
      "source": [
        "classifications_512 = model_512.predict(test_images)\n",
        "print('Clasificación con 512 neuronas:\\n', classifications_512[0])\n",
        "\n",
        "# Determinamos la clase predicha por el modelo de forma sencilla\n",
        "pred_512 = np.argmax(classifications_512[0])\n",
        "print('\\nClase predicha por el modelo con 512 neuronas en la capa oculta:', pred_512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Clasificación con 1024 neuronas:\n",
            " [4.7920821e-07 5.8147132e-10 1.3423413e-09 4.1538033e-11 3.0823365e-08 1.1502149e-03 3.6500109e-07 4.5721307e-03 2.5309375e-09 9.9427682e-01]\n",
            "\n",
            "Clase predicha por el modelo con 1024 neuronas en la capa oculta: 9\n"
          ]
        }
      ],
      "source": [
        "classifications_1024 = model_1024.predict(test_images)\n",
        "print('Clasificación con 1024 neuronas:\\n', classifications_1024[0])\n",
        "\n",
        "# Determinamos la clase predicha por el modelo de forma sencilla\n",
        "pred_1024 = np.argmax(classifications_1024[0])\n",
        "print('\\nClase predicha por el modelo con 1024 neuronas en la capa oculta:', pred_1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3NfwdOGZcAa"
      },
      "source": [
        "Como podemos ver tanto en el vector como (de forma más sencilla) en la salida al aplicar la función np.argmax para cada modelo con 512 y 1024 neuronas, el resultado es el mismo que para el modelo con 128 neuronas en la capa oculta. El modelo clasifica la primera imagen como la clase 9 del dataset Fashion MNIST."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFmfpxE1ZcJx"
      },
      "source": [
        "A medida que aumenta el número de neuronas en la capa oculta, el porcentaje de fiabilidad del modelo aumenta. En este caso, aumenta muy poco, ya que el modelo predice con gran seguridad (>99%) que la imagen pertenece a la clase tipo 9. Hay que tener en cuenta que un aumento excesivo del número de neuronas podría tener un mayor riesgo de sobreajuste del modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59eM76O1YekZ"
      },
      "source": [
        "# **5: Capa Flatten**\n",
        "\n",
        "En este ejercicio vamos a ver que ocurre cuando quitamos la capa flatten, para ello, escribe la red neuronal de la pregunta 1 y no pongas la capa Flatten.\n",
        "\n",
        "**pregunta 5 (0.5 puntos):** ¿puedes explicar por qué da el error que da?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecfEVKEuG4iU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None,), output.shape=(None, 28, 10)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[64], line 11\u001b[0m\n\u001b[0;32m      6\u001b[0m model_sinFlatten\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m                 loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m                 metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Entrenamos el modelo\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m entrenamiento \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_sinFlatten\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:619\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    614\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must be at least rank 1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    615\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    616\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    617\u001b[0m     )\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m--> 619\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    620\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must have rank (ndim) `target.ndim - 1`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    622\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    623\u001b[0m     )\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
            "\u001b[1;31mValueError\u001b[0m: Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None,), output.shape=(None, 28, 10)"
          ]
        }
      ],
      "source": [
        "# Definimos el modelo\n",
        "model_sinFlatten = Sequential([Dense(128, activation='relu'), # Capa oculta con 128 neuronas y activación relu\n",
        "                    Dense(10, activation='softmax')]) # Capa de salida con activación softmax para 10 clases\n",
        "\n",
        "# Compilamos el modelo\n",
        "model_sinFlatten.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Entrenamos el modelo\n",
        "entrenamiento = model_sinFlatten.fit(training_images, training_labels, epochs=10, batch_size=64, validation_data=(test_images, test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aNmrkkOZN6D"
      },
      "source": [
        "El 'ValueError' que recibimos tras quitar la capa Flatten al intentar construir nuestro modelo, el cual trabaja con imágenes, hace referencia a que no podemos conectar una capa Dense con las imágenes de entrada en formato matriz, ya que esta capa espera recibir de forma interna una única dimensión para cada muestra.\n",
        "\n",
        "En el dataset Fashion MNIST tenemos imágenes formadas por matrices de 28x28 píxeles. Sin una capa Flatten para transformar las matrices en vectores unidimensionales, las capas Dense de las capas ocultas y de salida no pueden procesar los datos.\n",
        "\n",
        "La capa Flatten es crucial en el procesamiento de imágenes con redes neuronales densas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f37cIr81ZYJj"
      },
      "source": [
        "# **6: Número de neuronas de la capa de salida**\n",
        "Considerad la capa final, la de salida de la red neuronal de la pregunta 1.\n",
        "\n",
        "**pregunta 6.1 (0.25 puntos)**: ¿Por qué son 10 las neuronas de la última capa?\n",
        "\n",
        "**pregunta 6.2 (0.25 puntos)**: ¿Qué pasaría si tuvieras una cantidad diferente a 10? \n",
        "\n",
        "Por ejemplo, intenta entrenar la red con 5, para ello utiliza la red neuronal de la pregunta 1 y cambia a 5 el número de neuronas en la última capa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhbZkppYZOCS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\asyncio\\events.py\", line 84, in _run\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 359, in execute_request\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 446, in do_execute\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\migue\\AppData\\Local\\Temp\\ipykernel_82756\\2510812629.py\", line 12, in <module>\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 314, in fit\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 117, in one_step_on_iterator\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 104, in one_step_on_data\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 54, in train_step\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py\", line 316, in compute_loss\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 609, in __call__\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 645, in call\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\losses\\loss.py\", line 43, in __call__\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\losses\\losses.py\", line 22, in call\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\losses\\losses.py\", line 1722, in sparse_categorical_crossentropy\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\ops\\nn.py\", line 1567, in sparse_categorical_crossentropy\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py\", line 638, in sparse_categorical_crossentropy\n\nReceived a label value of 9 which is outside the valid range of [0, 5).  Label values: 5 1 1 9 9 6 8 7 1 0 9 7 4 3 6 5 7 3 6 5 0 4 2 1 5 0 4 4 0 0 9 6 6 0 1 5 2 9 7 9 1 6 2 4 1 4 2 2 5 6 3 2 4 8 2 5 8 1 5 5 8 9 4 2\n\t [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_one_step_on_iterator_167140]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[29], line 12\u001b[0m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m                 loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m                 metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Entrenamos el modelo\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m entrenamiento \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\asyncio\\events.py\", line 84, in _run\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 359, in execute_request\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 446, in do_execute\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\migue\\AppData\\Local\\Temp\\ipykernel_82756\\2510812629.py\", line 12, in <module>\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 314, in fit\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 117, in one_step_on_iterator\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 104, in one_step_on_data\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 54, in train_step\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py\", line 316, in compute_loss\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 609, in __call__\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 645, in call\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\losses\\loss.py\", line 43, in __call__\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\losses\\losses.py\", line 22, in call\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\losses\\losses.py\", line 1722, in sparse_categorical_crossentropy\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\ops\\nn.py\", line 1567, in sparse_categorical_crossentropy\n\n  File \"c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py\", line 638, in sparse_categorical_crossentropy\n\nReceived a label value of 9 which is outside the valid range of [0, 5).  Label values: 5 1 1 9 9 6 8 7 1 0 9 7 4 3 6 5 7 3 6 5 0 4 2 1 5 0 4 4 0 0 9 6 6 0 1 5 2 9 7 9 1 6 2 4 1 4 2 2 5 6 3 2 4 8 2 5 8 1 5 5 8 9 4 2\n\t [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_one_step_on_iterator_167140]"
          ]
        }
      ],
      "source": [
        "# Definimos el modelo\n",
        "model_6 = Sequential([Flatten(input_shape=(28, 28)), # Aplanamos la entrada de imágenes 28x28 a un vector\n",
        "                    Dense(128, activation='relu'), # Capa oculta con 128 neuronas y activación relu\n",
        "                    Dense(5, activation='softmax')]) # Capa de salida con activación softmax para 10 clases\n",
        "\n",
        "# Compilamos el modelo\n",
        "model_6.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Entrenamos el modelo\n",
        "entrenamiento = model_6.fit(training_images, training_labels, epochs=10, batch_size=64, validation_data=(test_images, test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLsQcq-6aUoD"
      },
      "source": [
        "La última capa de nuestra red neuronal tiene que tener 10 neuronas porque corresponde con el número de clases distintas del dataset (10 tipos de prendas).\n",
        "Cada neurona en la capa de salida corresponde a una de estas clases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1f_7ZFeaUu6"
      },
      "source": [
        "Cuando cambiamos el número de neuronas en la última capa de nuestra red por 5 (u otro número distinto de 10), el modelo sólo podrá clasificar entre ese número de categorías de ropa posibles. Esto causa un error de ejecución porque las categorías del dataset siguen siendo 10 y las salidas de nuestro modelo sólo 5, por lo que obtenemos una discrepancia entre etiquetas y salidas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNIBCkshaf2y"
      },
      "source": [
        "# 7: Aumento de epoch y su efecto en la red neuronal\n",
        "En este ejercicio vamos a ver el impacto de aumentar los epoch en el entrenamiento. Usando la red neuronal de la pregunta 1:\n",
        "\n",
        "**pregunta 7.1 (0.15 puntos)**\n",
        "* Intentad 15 epoch para su entrenamiento, probablemente obtendras un modelo con una pérdida mucho mejor que el que tiene 10.\n",
        "\n",
        "**pregunta 7.2 (0.15 puntos)**\n",
        "* Intenta ahora con 30 epoch para su entrenamiento, podrás ver que el valor de la pérdida deja de disminuir, y a veces aumenta.\n",
        "\n",
        "**pregunta 7.3 (0.30 puntos)**\n",
        "* ¿Por qué piensas que ocurre esto? Explica tu respuesta y da el nombre de este efecto si lo conoces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cb5vk_imG4iZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7731 - loss: 0.6595 - val_accuracy: 0.8482 - val_loss: 0.4304\n",
            "Epoch 2/15\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8601 - loss: 0.3962 - val_accuracy: 0.8556 - val_loss: 0.4120\n",
            "Epoch 3/15\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8728 - loss: 0.3519 - val_accuracy: 0.8668 - val_loss: 0.3665\n",
            "Epoch 4/15\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8832 - loss: 0.3228 - val_accuracy: 0.8704 - val_loss: 0.3589\n",
            "Epoch 5/15\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8897 - loss: 0.3047 - val_accuracy: 0.8680 - val_loss: 0.3614\n",
            "Epoch 6/15\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8957 - loss: 0.2831 - val_accuracy: 0.8753 - val_loss: 0.3480\n",
            "Epoch 7/15\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8967 - loss: 0.2761 - val_accuracy: 0.8731 - val_loss: 0.3575\n",
            "Epoch 8/15\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9039 - loss: 0.2644 - val_accuracy: 0.8809 - val_loss: 0.3329\n",
            "Epoch 9/15\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9080 - loss: 0.2498 - val_accuracy: 0.8789 - val_loss: 0.3428\n",
            "Epoch 10/15\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9117 - loss: 0.2408 - val_accuracy: 0.8854 - val_loss: 0.3270\n",
            "Epoch 11/15\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9153 - loss: 0.2334 - val_accuracy: 0.8828 - val_loss: 0.3344\n",
            "Epoch 12/15\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9175 - loss: 0.2269 - val_accuracy: 0.8812 - val_loss: 0.3443\n",
            "Epoch 13/15\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9175 - loss: 0.2228 - val_accuracy: 0.8893 - val_loss: 0.3235\n",
            "Epoch 14/15\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9209 - loss: 0.2159 - val_accuracy: 0.8857 - val_loss: 0.3311\n",
            "Epoch 15/15\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9216 - loss: 0.2100 - val_accuracy: 0.8888 - val_loss: 0.3231\n"
          ]
        }
      ],
      "source": [
        "# Definimos el modelo\n",
        "model_15epoch = Sequential([Flatten(input_shape=(28, 28)), # Aplanamos la entrada de imágenes 28x28 a un vector\n",
        "                    Dense(128, activation='relu'), # Capa oculta con 128 neuronas y activación relu\n",
        "                    Dense(10, activation='softmax')]) # Capa de salida con activación softmax para 10 clases\n",
        "\n",
        "# Compilamos el modelo\n",
        "model_15epoch.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Entrenamos el modelo\n",
        "entrenamiento = model_15epoch.fit(training_images, training_labels, epochs=15, batch_size=64, validation_data=(test_images, test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9jQ26Gda5cv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7703 - loss: 0.6656 - val_accuracy: 0.8229 - val_loss: 0.4872\n",
            "Epoch 2/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8561 - loss: 0.4016 - val_accuracy: 0.8599 - val_loss: 0.4023\n",
            "Epoch 3/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8744 - loss: 0.3528 - val_accuracy: 0.8711 - val_loss: 0.3715\n",
            "Epoch 4/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8828 - loss: 0.3208 - val_accuracy: 0.8615 - val_loss: 0.3795\n",
            "Epoch 5/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8875 - loss: 0.3083 - val_accuracy: 0.8751 - val_loss: 0.3492\n",
            "Epoch 6/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8942 - loss: 0.2872 - val_accuracy: 0.8720 - val_loss: 0.3595\n",
            "Epoch 7/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8978 - loss: 0.2788 - val_accuracy: 0.8672 - val_loss: 0.3611\n",
            "Epoch 8/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9027 - loss: 0.2624 - val_accuracy: 0.8791 - val_loss: 0.3368\n",
            "Epoch 9/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9043 - loss: 0.2586 - val_accuracy: 0.8756 - val_loss: 0.3485\n",
            "Epoch 10/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9092 - loss: 0.2456 - val_accuracy: 0.8765 - val_loss: 0.3651\n",
            "Epoch 11/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9110 - loss: 0.2409 - val_accuracy: 0.8800 - val_loss: 0.3328\n",
            "Epoch 12/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9155 - loss: 0.2317 - val_accuracy: 0.8786 - val_loss: 0.3425\n",
            "Epoch 13/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9187 - loss: 0.2224 - val_accuracy: 0.8876 - val_loss: 0.3267\n",
            "Epoch 14/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9200 - loss: 0.2160 - val_accuracy: 0.8833 - val_loss: 0.3390\n",
            "Epoch 15/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9226 - loss: 0.2095 - val_accuracy: 0.8877 - val_loss: 0.3373\n",
            "Epoch 16/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9244 - loss: 0.2057 - val_accuracy: 0.8858 - val_loss: 0.3316\n",
            "Epoch 17/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9273 - loss: 0.1957 - val_accuracy: 0.8893 - val_loss: 0.3320\n",
            "Epoch 18/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9289 - loss: 0.1895 - val_accuracy: 0.8859 - val_loss: 0.3422\n",
            "Epoch 19/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9313 - loss: 0.1829 - val_accuracy: 0.8782 - val_loss: 0.3621\n",
            "Epoch 20/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9324 - loss: 0.1815 - val_accuracy: 0.8844 - val_loss: 0.3446\n",
            "Epoch 21/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9342 - loss: 0.1792 - val_accuracy: 0.8832 - val_loss: 0.3580\n",
            "Epoch 22/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9370 - loss: 0.1697 - val_accuracy: 0.8874 - val_loss: 0.3542\n",
            "Epoch 23/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9354 - loss: 0.1722 - val_accuracy: 0.8886 - val_loss: 0.3628\n",
            "Epoch 24/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9375 - loss: 0.1699 - val_accuracy: 0.8906 - val_loss: 0.3443\n",
            "Epoch 25/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9392 - loss: 0.1615 - val_accuracy: 0.8877 - val_loss: 0.3628\n",
            "Epoch 26/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9431 - loss: 0.1545 - val_accuracy: 0.8904 - val_loss: 0.3549\n",
            "Epoch 27/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9449 - loss: 0.1500 - val_accuracy: 0.8884 - val_loss: 0.3554\n",
            "Epoch 28/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9449 - loss: 0.1467 - val_accuracy: 0.8896 - val_loss: 0.3609\n",
            "Epoch 29/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9444 - loss: 0.1467 - val_accuracy: 0.8890 - val_loss: 0.3593\n",
            "Epoch 30/30\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9470 - loss: 0.1434 - val_accuracy: 0.8837 - val_loss: 0.3850\n"
          ]
        }
      ],
      "source": [
        "# Definimos el modelo\n",
        "model_30epoch = Sequential([Flatten(input_shape=(28, 28)), # Aplanamos la entrada de imágenes 28x28 a un vector\n",
        "                    Dense(128, activation='relu'), # Capa oculta con 128 neuronas y activación relu\n",
        "                    Dense(10, activation='softmax')]) # Capa de salida con activación softmax para 10 clases\n",
        "\n",
        "# Compilamos el modelo\n",
        "model_30epoch.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Entrenamos el modelo\n",
        "entrenamiento = model_30epoch.fit(training_images, training_labels, epochs=30, batch_size=64, validation_data=(test_images, test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs0fjzH4bmSR"
      },
      "source": [
        "Al aumentar el número de epoch en el entrenamiento de una red neuronal, permitimos que el modelo tenga más oportunidades de aprender de los datos. Es importante mantener un número de epoch ajustado a las necesidades del modelo, dependiendo del tamaño del dataset y la calidad de los datos.\n",
        "\n",
        "Como se comenta en el enunciado, al incrementar el número de epoch de 10 a 15 vemos una mejora en el rendimiento del modelo, porque éste tiene más iteraciones para ajustar los pesos basándose en los datos de entrenamiento y reduciendo la pérdida.\n",
        "\n",
        "Al aumentar a 30 epoch podemos observar como el valor de pérdidas disminuye de forma mucho más lenta, incluso en algunos casos (como en la epoch 22-23) puede aumentar.\n",
        "\n",
        "El efecto más común en estos casos es el sobreajuste u overfitting, que hace referencia a cuando un modelo se ajusta demasiado bien a los datos de entrenamiento y disminuye su capacidad de clasificar otros nuevos datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlIgNG4Yb_N6"
      },
      "source": [
        "# 8: Early stop\n",
        "En el ejercicio anterior, cuando entrenabas con epoch extras, tenías un problema en el que tu pérdida podía cambiar. Puede que te haya llevado un poco de tiempo esperar a que el entrenamiento lo hiciera,  y puede que hayas pensado \"¿no estaría bien si pudiera parar el entrenamiento cuando alcance un valor deseado?\", es decir, una precisión del 85% podría ser suficiente para ti, y si alcanzas eso después de 3 epoch, ¿por qué sentarte a esperar a que termine muchas más épocas? Como cualquier otro programa existen formas de parar la ejecución\n",
        "\n",
        "A partir del código de ejemplo, hacer una nueva función que tenga en cuenta la perdida (loss) y que pueda parar el código para evitar que ocurra el efeto secundario que vimos en el ejercicio 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5UwceFUG4ic"
      },
      "outputs": [],
      "source": [
        "### Ejemplo de código\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "      def on_epoch_end(self, epoch, logs={}):\n",
        "            if(logs.get('accuracy')> 0.85):\n",
        "                  print(\"\\nAlcanzado el 85% de precisión, se cancela el entrenamiento!!\")\n",
        "                  self.model.stop_training = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bjd8wGKccrn"
      },
      "source": [
        "**Ejercicio 8 *(0.75 puntos)***: Completa el siguiente código con una clase callback que una vez alcanzado el 40% de perdida detenga el entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos una clase 'myCallback' que hereda de 'tf.keras.callbacks.Callback'. Definimos un método 'on_epoch_end' para que se ejecute al final de cada epoch si se verifica que la pérdida (loss) es menor o igual al 40% (0.4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29LSfdOvc270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.16.1\n",
            "Epoch 1/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7935 - loss: 0.5827\n",
            "Epoch 2/50\n",
            "\u001b[1m1863/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8659 - loss: 0.3673\n",
            "Pérdida menor o igual al 40%. Cancelando el entrenamiento...\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8659 - loss: 0.3672\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x19990cd8d50>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "# Completamos el código con una clase callback tomando como ejemplo la celda anterior, pero modificando las condiciones dentro del if:\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        # Comprobamos que la pérdida sea menor o igual al 40%:\n",
        "        if logs.get('loss') <= 0.4:\n",
        "            print('\\nPérdida menor o igual al 40%. Cancelando el entrenamiento...')\n",
        "            self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images = training_images/255.0\n",
        "test_images = test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "model.compile(optimizer = 'adam',\n",
        "            loss = 'sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']) \n",
        "\n",
        "model.fit(training_images, training_labels, epochs=50, callbacks=[callbacks])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos observar en los resultados, el entrenamiento se ha detenido en el segundo epoch, cuando el valor de pérdida es inferior a 0.4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_yZ9B8gTFqR"
      },
      "source": [
        "## 9. Unidades de activación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuVNxmXSTFqR"
      },
      "source": [
        "En este ejercicio, vamos a evaluar la importancia de utilizar las unidades de activación adecuadas. Como hemos visto en clase, funciones de activación como sigmoid han dejado de utilizarse en favor de otras unidades como ReLU.\n",
        "\n",
        "**Ejercicio 9 *(0.75 puntos)***: Partiendo de una red sencilla como la desarrollada en el Trabajo 1, escribir un breve análisis comparando la utilización de unidades sigmoid y ReLU (por ejemplo, se pueden comentar aspectos como velocidad de convergencia, métricas obtenidas...). Explicar por qué pueden darse estas diferencias. Opcionalmente, comparar con otras activaciones disponibles en Keras.\n",
        "\n",
        "*Pista: Usando redes más grandes se hace más sencillo apreciar las diferencias. Es mejor utilizar al menos 3 o 4 capas densas.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En primer lugar, utilizaremos un modelo con función de activación sigmoid:\n",
        "\n",
        "La función de activación sigmoid limita su salida a un rango entre 0 y 1, lo que la hace adecuada para problemas de clasificación binaria y estadística. Sin embargo, presenta algunas desventajas cuando se utiliza en redes profundas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoYUajTuTFqS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.6183 - loss: 1.0855 - val_accuracy: 0.8315 - val_loss: 0.4577\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8459 - loss: 0.4231 - val_accuracy: 0.8570 - val_loss: 0.3995\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8647 - loss: 0.3725 - val_accuracy: 0.8628 - val_loss: 0.3711\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8737 - loss: 0.3477 - val_accuracy: 0.8701 - val_loss: 0.3579\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8805 - loss: 0.3280 - val_accuracy: 0.8762 - val_loss: 0.3366\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8911 - loss: 0.3001 - val_accuracy: 0.8712 - val_loss: 0.3421\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8923 - loss: 0.2901 - val_accuracy: 0.8758 - val_loss: 0.3401\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8982 - loss: 0.2762 - val_accuracy: 0.8816 - val_loss: 0.3276\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8984 - loss: 0.2743 - val_accuracy: 0.8828 - val_loss: 0.3200\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9053 - loss: 0.2550 - val_accuracy: 0.8888 - val_loss: 0.3120\n"
          ]
        }
      ],
      "source": [
        "# Definimos el modelo con función de activación sigmoid\n",
        "model_sigmoid = Sequential([Flatten(input_shape=(28,28)),\n",
        "                            Dense(128, activation='sigmoid'),\n",
        "                            Dense(128, activation='sigmoid'),\n",
        "                            Dense(128, activation='sigmoid'),\n",
        "                            Dense(10, activation='softmax')])\n",
        "\n",
        "model_sigmoid.compile(optimizer='adam',\n",
        "                    loss='sparse_categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "entrenamiento_sigmoid = model_sigmoid.fit(training_images, training_labels, epochs=10, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pasamos al modelo con función de activación ReLU:\n",
        "\n",
        "Este tipo de función de activación resuelve algunos problemas de las funciones sigmoid en redes profundas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7695 - loss: 0.6489 - val_accuracy: 0.8296 - val_loss: 0.4580\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8574 - loss: 0.3873 - val_accuracy: 0.8487 - val_loss: 0.4016\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8724 - loss: 0.3406 - val_accuracy: 0.8740 - val_loss: 0.3527\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8858 - loss: 0.3141 - val_accuracy: 0.8816 - val_loss: 0.3259\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8877 - loss: 0.2999 - val_accuracy: 0.8827 - val_loss: 0.3348\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8928 - loss: 0.2842 - val_accuracy: 0.8831 - val_loss: 0.3306\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8973 - loss: 0.2679 - val_accuracy: 0.8672 - val_loss: 0.3805\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9064 - loss: 0.2506 - val_accuracy: 0.8850 - val_loss: 0.3190\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9089 - loss: 0.2424 - val_accuracy: 0.8755 - val_loss: 0.3439\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9093 - loss: 0.2382 - val_accuracy: 0.8847 - val_loss: 0.3405\n"
          ]
        }
      ],
      "source": [
        "# Definimos el modelo con función de activación sigmoid\n",
        "model_relu = Sequential([Flatten(input_shape=(28,28)),\n",
        "                            Dense(128, activation='relu'),\n",
        "                            Dense(128, activation='relu'),\n",
        "                            Dense(128, activation='relu'),\n",
        "                            Dense(10, activation='softmax')])\n",
        "\n",
        "model_relu.compile(optimizer='adam',\n",
        "                    loss='sparse_categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "entrenamiento_sigmoid = model_relu.fit(training_images, training_labels, epochs=10, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tras analizar los resultados de ambos modelos utilizando las funciones sigmoid y relu, podemos concluir que:\n",
        "\n",
        "· La velocidad de convergencia en nuestro ensayo es la misma en todas las etapas de los modelos, aunque ReLU tiene a converger más rápido que sigmoid ya que no satura en el rango positivo y reduce el problema del gradiente desvaneciente, haciendo que los gradientes se propaguen hacia atrás en la red de manera más eficiente.\n",
        "\n",
        "· El rendimiento del modelo es ligeramente superior en el modelo con función de activación ReLU, ya que si nos fijamos en las métricas, los valores de precisión y pérdida son superiores e inferiores respectivamente al modelo con sigmoid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu6RbUFKTFqT"
      },
      "source": [
        "## 10. Inicialización de parámetros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abmm05UPTFqU"
      },
      "source": [
        "En este ejercicio, vamos a evaluar la importancia de una correcta inicialización de parámetros en una red neuronal.\n",
        "\n",
        "**Ejercicio 10 *(0.75 puntos)***: Partiendo de una red similar a la del ejercicio anterior (usando ya ReLUs), comentar las diferencias que se aprecian en el entrenamiento al utilizar distintas estrategias de inicialización de parámetros. Para ello, inicializar todas las capas con las siguientes estrategias, disponibles en Keras, y analizar sus diferencias:\n",
        "\n",
        "* Inicialización con ceros.\n",
        "* Inicialización con una variable aleatoria normal.\n",
        "* Inicialización con los valores por defecto de Keras para una capa Dense (estrategia *glorot uniform*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos un modelo base para clasificar las imágenes del dataset con diferentes inicializadores. Para utilizar los inicializadores, debemos importar las bibliotecas de TensorFlow correspondientes a los initializers que queremos utilizar, en nuestro caso: Zeros, RandomNormal y GlorotUniform. Añadimos al inicio: from tensorflow.keras.initializers import Zeros, RandomNormal, GlorotUniform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcMt7pSkTFqU"
      },
      "outputs": [],
      "source": [
        "def create_model(inicializador):\n",
        "    model_base = Sequential([Flatten(input_shape=(28,28)),\n",
        "                            Dense(128, activation='relu', kernel_initializer=inicializador),\n",
        "                            Dense(128, activation='relu', kernel_initializer=inicializador),\n",
        "                            Dense(128, activation='relu', kernel_initializer=inicializador),\n",
        "                            Dense(10, activation='softmax')])\n",
        "    \n",
        "    model_base.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    return model_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inicialización con ceros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9043 - loss: 0.2578 - val_accuracy: 0.9115 - val_loss: 0.2443\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9084 - loss: 0.2441 - val_accuracy: 0.8975 - val_loss: 0.2740\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9104 - loss: 0.2376 - val_accuracy: 0.9031 - val_loss: 0.2602\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9146 - loss: 0.2267 - val_accuracy: 0.9041 - val_loss: 0.2575\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9223 - loss: 0.2106 - val_accuracy: 0.9039 - val_loss: 0.2701\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9221 - loss: 0.2070 - val_accuracy: 0.8928 - val_loss: 0.2965\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9235 - loss: 0.2035 - val_accuracy: 0.8965 - val_loss: 0.2846\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9265 - loss: 0.1963 - val_accuracy: 0.9072 - val_loss: 0.2555\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9281 - loss: 0.1880 - val_accuracy: 0.9014 - val_loss: 0.2762\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9293 - loss: 0.1851 - val_accuracy: 0.9032 - val_loss: 0.2771\n"
          ]
        }
      ],
      "source": [
        "model_ceros = create_model(Zeros())\n",
        "entrenamiento_ceros = model_ceros.fit(training_images, training_labels, epochs=10, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inicialización con una distribución Normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9331 - loss: 0.1805 - val_accuracy: 0.9003 - val_loss: 0.2879\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9348 - loss: 0.1755 - val_accuracy: 0.8982 - val_loss: 0.2957\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9350 - loss: 0.1736 - val_accuracy: 0.9004 - val_loss: 0.2873\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9367 - loss: 0.1713 - val_accuracy: 0.8953 - val_loss: 0.2981\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9403 - loss: 0.1590 - val_accuracy: 0.9027 - val_loss: 0.2924\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9435 - loss: 0.1521 - val_accuracy: 0.9011 - val_loss: 0.2928\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9427 - loss: 0.1510 - val_accuracy: 0.8917 - val_loss: 0.3453\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9466 - loss: 0.1451 - val_accuracy: 0.8988 - val_loss: 0.3147\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9454 - loss: 0.1481 - val_accuracy: 0.8979 - val_loss: 0.3180\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9472 - loss: 0.1439 - val_accuracy: 0.8994 - val_loss: 0.3152\n"
          ]
        }
      ],
      "source": [
        "model_Normal = create_model(RandomNormal(mean=0.0, stddev=0.05)) # Valores de distribución normal\n",
        "entrenamiento_Normal = model_Normal.fit(training_images, training_labels, epochs=10, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inicialización con una distribución Gloriot Uniform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9471 - loss: 0.1428 - val_accuracy: 0.8993 - val_loss: 0.3185\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9479 - loss: 0.1379 - val_accuracy: 0.9020 - val_loss: 0.3258\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9517 - loss: 0.1347 - val_accuracy: 0.8973 - val_loss: 0.3402\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9517 - loss: 0.1303 - val_accuracy: 0.8970 - val_loss: 0.3579\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9520 - loss: 0.1266 - val_accuracy: 0.9008 - val_loss: 0.3351\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9553 - loss: 0.1212 - val_accuracy: 0.8976 - val_loss: 0.3336\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9553 - loss: 0.1196 - val_accuracy: 0.9014 - val_loss: 0.3316\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9570 - loss: 0.1158 - val_accuracy: 0.8989 - val_loss: 0.3441\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9583 - loss: 0.1133 - val_accuracy: 0.9001 - val_loss: 0.3461\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9589 - loss: 0.1142 - val_accuracy: 0.9001 - val_loss: 0.3539\n"
          ]
        }
      ],
      "source": [
        "model_GlorotUniform = create_model(GlorotUniform())\n",
        "entrenamiento_GlorotUniform = model_GlorotUniform.fit(training_images, training_labels, epochs=10, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analizando los resultados, parece que la función de activación más conveniente para el modelo es la GloriotUniform, ya que los tiempos de convergencia son todos iguales pero las métricas son ligeramente mejores en este último caso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqIAyVWrTFqV"
      },
      "source": [
        "## 11. Optimizadores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcYj29hYTFqW"
      },
      "source": [
        "**Ejercicio 11 *(0.75 puntos)***: Partiendo de una red similar a la del ejercicio anterior (utilizando la mejor estrategia de inicialización observada), comparar y analizar las diferencias que se observan  al entrenar con varios de los optimizadores vistos en clase, incluyendo SGD como optimizador básico (se puede explorar el espacio de hiperparámetros de cada optimizador, aunque para optimizadores más avanzados del estilo de adam y RMSprop es buena idea dejar los valores por defecto provistos por Keras)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como la mejor estrategia de inicialización observada en el apartado anterior ha sido Gloriot Uniform, pasamos a experimentar con este modelo utilizando optimizadores SGD, Adam y RMSprop.\n",
        "\n",
        "Primero, configuramos un modelo base igual que antes, al que pasaremos como parámetro del método el optimizador correspondiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "0fWDiqXvTFqW"
      },
      "outputs": [],
      "source": [
        "def create_model2(optimizador):\n",
        "    model_base2 = Sequential([Flatten(input_shape=(28,28)),\n",
        "                            Dense(128, activation='relu', kernel_initializer=GlorotUniform()),\n",
        "                            Dense(128, activation='relu', kernel_initializer=GlorotUniform()),\n",
        "                            Dense(128, activation='relu', kernel_initializer=GlorotUniform()),\n",
        "                            Dense(10, activation='softmax')])\n",
        "    \n",
        "    model_base2.compile(optimizer=optimizador,\n",
        "                        loss='sparse_categorical_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "    \n",
        "    return model_base2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimizador SGD - Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.6242 - loss: 1.1500 - val_accuracy: 0.8143 - val_loss: 0.5376\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8173 - loss: 0.5236 - val_accuracy: 0.8278 - val_loss: 0.4781\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8368 - loss: 0.4610 - val_accuracy: 0.8403 - val_loss: 0.4494\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8474 - loss: 0.4283 - val_accuracy: 0.8447 - val_loss: 0.4333\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8614 - loss: 0.3954 - val_accuracy: 0.8598 - val_loss: 0.4018\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8641 - loss: 0.3826 - val_accuracy: 0.8614 - val_loss: 0.3918\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8688 - loss: 0.3648 - val_accuracy: 0.8689 - val_loss: 0.3749\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8714 - loss: 0.3556 - val_accuracy: 0.8665 - val_loss: 0.3779\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8753 - loss: 0.3492 - val_accuracy: 0.8732 - val_loss: 0.3590\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8780 - loss: 0.3381 - val_accuracy: 0.8711 - val_loss: 0.3665\n"
          ]
        }
      ],
      "source": [
        "model_SGD = create_model2('SGD')\n",
        "entrenamiento_SGD = model_SGD.fit(training_images, training_labels, epochs=10, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimizador Adam\n",
        "\n",
        "Utilizado anteriormente en la práctica. Se basa en la estimación adaptativa de momentos de primer y segundo orden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7653 - loss: 0.6601 - val_accuracy: 0.8487 - val_loss: 0.4069\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8564 - loss: 0.3835 - val_accuracy: 0.8617 - val_loss: 0.3796\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8757 - loss: 0.3352 - val_accuracy: 0.8686 - val_loss: 0.3642\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8826 - loss: 0.3202 - val_accuracy: 0.8707 - val_loss: 0.3561\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8908 - loss: 0.2907 - val_accuracy: 0.8794 - val_loss: 0.3344\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8942 - loss: 0.2784 - val_accuracy: 0.8658 - val_loss: 0.3770\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9001 - loss: 0.2673 - val_accuracy: 0.8804 - val_loss: 0.3373\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9021 - loss: 0.2600 - val_accuracy: 0.8904 - val_loss: 0.3092\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9080 - loss: 0.2421 - val_accuracy: 0.8875 - val_loss: 0.3324\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9111 - loss: 0.2347 - val_accuracy: 0.8857 - val_loss: 0.3430\n"
          ]
        }
      ],
      "source": [
        "model_adam = create_model2('adam')\n",
        "entrenamiento_adam = model_adam.fit(training_images, training_labels, epochs=10, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimizador RMSprop\n",
        "\n",
        "Este optimizador ajusta la tasa de aprendizaje de manera adaptativa, diseñado para resolver problemas de convergencia rápida AdaGrad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Anaconda\\envs\\Sist_Cognitivos_Artificiales\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7523 - loss: 0.6861 - val_accuracy: 0.8390 - val_loss: 0.4467\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8540 - loss: 0.4033 - val_accuracy: 0.8570 - val_loss: 0.4216\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8693 - loss: 0.3675 - val_accuracy: 0.8420 - val_loss: 0.4885\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8733 - loss: 0.3572 - val_accuracy: 0.8734 - val_loss: 0.3680\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8775 - loss: 0.3470 - val_accuracy: 0.8673 - val_loss: 0.4654\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8788 - loss: 0.3450 - val_accuracy: 0.8565 - val_loss: 0.4617\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8820 - loss: 0.3372 - val_accuracy: 0.8708 - val_loss: 0.4069\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8814 - loss: 0.3359 - val_accuracy: 0.8562 - val_loss: 0.4495\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8852 - loss: 0.3351 - val_accuracy: 0.8727 - val_loss: 0.4461\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8863 - loss: 0.3352 - val_accuracy: 0.8758 - val_loss: 0.4299\n"
          ]
        }
      ],
      "source": [
        "model_rmsprop = create_model2('rmsprop')\n",
        "entrenamiento_rmsprop = model_rmsprop.fit(training_images, training_labels, epochs=10, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analizando los resultados, podemos comprobar que los optimizadores SGD y RMSprop son un poco más rápidos en el primer epoch, aunque para el resto de estapas la velocidad de convergencia es la misma en todos los casos. El rendimiento del modelo con los tres optimizadores es muy similar, sobresaliendo el Adam por encima de los otros dos, aunque no existan grandes diferencias. Podemos resaltar el optimizador Adam por encima del resto, dado que sus métricas son mejores y la velocidad de convergencia se ajusta perfectamente a las necesidades."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkfTFoJOTFqZ"
      },
      "source": [
        "## 12. Regularización y red final *(1.25 puntos)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6CQhK7ZTFqZ"
      },
      "source": [
        "**Ejercicio 12 *(1 punto)***: Entrenar una red final que sea capaz de obtener una accuracy en el validation set cercana al 90%. Para ello, combinar todo lo aprendido anteriormente y utilizar técnicas de regularización para evitar overfitting. Algunos de los elementos que pueden tenerse en cuenta son los siguientes.\n",
        "\n",
        "* Número de capas y neuronas por capa\n",
        "* Optimizadores y sus parámetros\n",
        "* Batch size\n",
        "* Unidades de activación\n",
        "* Uso de capas dropout, regularización L2, regularización L1...\n",
        "* Early stopping (se puede aplicar como un callback de Keras, o se puede ver un poco \"a ojo\" cuándo el modelo empieza a caer en overfitting y seleccionar el número de epochs necesarias)\n",
        "* Batch normalization\n",
        "\n",
        "Si los modelos entrenados anteriormente ya se acercaban al valor requerido de accuracy, probar distintas estrategias igualmente y comentar los resultados.\n",
        "\n",
        "Explicar brevemente la estrategia seguida y los modelos probados para obtener el modelo final, que debe verse entrenado en este Notebook. No es necesario guardar el entrenamiento de todos los modelos que se han probado, es suficiente con explicar cómo se ha llegado al modelo final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configuramos nuestro modelo en base a los resultados obtenidos en los pasos anteriores del ejercicio y en las mejores prácticas actuales para lograr entrenar la red neuronal con el objetivo de alcanzar una precisión cercana al 90% en el conjunto de validación del dataset Fashion MNIST.\n",
        "\n",
        "· Utilizaremos varias capas densas con distinto número de neuronas en las capas ocultas y funciones de activación ReLU, que ha demostrado ser la más eficiente en los casos anteriores.\n",
        "\n",
        "· Optimizador Adam por su rendimiento probado en el apartado anterior y la gestión automática de la tasa de aprendizaje.\n",
        "\n",
        "· Incorporaremos Dropout para reducir el overfitting al 'matar' algunas neuronas de forma aleatoria durante el entrenamiento.\n",
        "\n",
        "· Aplicaremos inicialización GlorotUniform en las capas densas ya que han demostrado obtener los mejores resultados en cuanto a rendimiento.\n",
        "\n",
        "· Utilizaremos capas de Batch Normalization después de cada capa de activación para estabilizar y acelerar el entrenamiento.\n",
        "\n",
        "· Aplicamos un callback de Early Stopping para detener el entrenamiento cuando la precisión en el conjunto de validación deje de mejorar, evitando así el sobreajuste.\n",
        "\n",
        "· Seleccionamos un Batch Size adecuado y un número de epochs basado en las observaciones previas del entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "AUJ5AtunTFqa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7796 - loss: 0.6299 - val_accuracy: 0.8376 - val_loss: 0.4178\n",
            "Epoch 2/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8508 - loss: 0.4076 - val_accuracy: 0.8572 - val_loss: 0.3880\n",
            "Epoch 3/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8622 - loss: 0.3711 - val_accuracy: 0.8652 - val_loss: 0.3694\n",
            "Epoch 4/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8722 - loss: 0.3532 - val_accuracy: 0.8712 - val_loss: 0.3521\n",
            "Epoch 5/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8800 - loss: 0.3265 - val_accuracy: 0.8605 - val_loss: 0.3675\n",
            "Epoch 6/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8797 - loss: 0.3225 - val_accuracy: 0.8702 - val_loss: 0.3498\n",
            "Epoch 7/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8846 - loss: 0.3085 - val_accuracy: 0.8702 - val_loss: 0.3600\n",
            "Epoch 8/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8881 - loss: 0.2950 - val_accuracy: 0.8737 - val_loss: 0.3386\n",
            "Epoch 9/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8936 - loss: 0.2874 - val_accuracy: 0.8640 - val_loss: 0.3672\n",
            "Epoch 10/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8946 - loss: 0.2795 - val_accuracy: 0.8827 - val_loss: 0.3191\n",
            "Epoch 11/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8959 - loss: 0.2793 - val_accuracy: 0.8776 - val_loss: 0.3386\n",
            "Epoch 12/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8988 - loss: 0.2695 - val_accuracy: 0.8850 - val_loss: 0.3198\n",
            "Epoch 13/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8991 - loss: 0.2704 - val_accuracy: 0.8932 - val_loss: 0.3025\n",
            "Epoch 14/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9039 - loss: 0.2560 - val_accuracy: 0.8779 - val_loss: 0.3381\n",
            "Epoch 15/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9054 - loss: 0.2495 - val_accuracy: 0.8836 - val_loss: 0.3252\n",
            "Epoch 16/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9080 - loss: 0.2457 - val_accuracy: 0.8859 - val_loss: 0.3212\n",
            "Epoch 17/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9111 - loss: 0.2393 - val_accuracy: 0.8913 - val_loss: 0.3123\n",
            "Epoch 18/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9117 - loss: 0.2369 - val_accuracy: 0.8875 - val_loss: 0.3210\n"
          ]
        }
      ],
      "source": [
        "final_model = Sequential([Flatten(input_shape=(28,28)),\n",
        "                        Dense(512, activation='relu', kernel_initializer=GlorotUniform()),\n",
        "                        BatchNormalization(),\n",
        "                        Dropout(0.1),\n",
        "                        Dense(256, activation='relu', kernel_initializer=GlorotUniform()),\n",
        "                        BatchNormalization(),\n",
        "                        Dropout(0.1),\n",
        "                        Dense(128, activation='relu', kernel_initializer=GlorotUniform()),\n",
        "                        BatchNormalization(),\n",
        "                        Dropout(0.1),\n",
        "                        Dense(10, activation='softmax')])\n",
        "\n",
        "final_model.compile(optimizer='adam',\n",
        "                    loss='sparse_categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "entrenamiento_final = final_model.fit(training_images, training_labels, epochs=50, validation_split=0.2, batch_size=64, callbacks=[EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En las pruebas anteriores, el modelo que mejores métricas había obtenido era cuando configurabamos un inicializador GlorotUniform, con el cual el modelo alcanzaba unos valores de accuracy superiores al 90%. Cuando en lugar de un inilizador hemos empleado una regularización L2 en las capas densas, no hemos obtenido mejores resultados que los que arroja este modelo. Por ello, he decidido prescindir de regularización e introducir nuevamente el inicializador que nos ha arrojado mejores resultados. De todos modos, el modelo anterior sigue arrojando mejores métricas que el actual, por lo que para un caso como el dataset Fashion MNIST, es probable que debamos escoger los resultados obtenidos en el apartado 10.3 (o incluso 10.2)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Actividad 1: Redes Neuronales.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
